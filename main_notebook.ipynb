{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from sklearnex import patch_sklearn\n",
    "# patch_sklearn(global_patch=True)\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import numpy\n",
    "import scipy \n",
    "\n",
    "def monkeypath_itemfreq(sampler_indices):\n",
    "\treturn zip(*numpy.unique(sampler_indices, return_counts=True))\n",
    "scipy.stats.itemfreq=monkeypath_itemfreq\n",
    "\n",
    "import textattack\n",
    "import transformers\n",
    "\n",
    "from utils import RANDOM_BASELINE_Attack, ADV_XAI_Attack\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:24'\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = 'cuda_malloc_async'\n",
    "\n",
    "\n",
    "\n",
    "from common import *\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def save(results, filename):\n",
    "\twith open('{}/results.pickle'.format(filename), 'wb') as handle:\n",
    "\t\tpickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\t\tprint(\"UPDATED TO\", filename)\n",
    "\n",
    "def load(filename):\n",
    "\tresults = None\n",
    "\ttry:\n",
    "\t\tresults = pickle.load(open(f\"{filename}/results.pickle\", 'rb'))\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\treturn results\n",
    "\n",
    "\n",
    "args = load_args()\n",
    "filename = generate_filename(args)\n",
    "\n",
    "print(\"+++++++++++++++++++++++++++++++++++\")\n",
    "print(filename)\n",
    "print(args)\n",
    "print(\"+++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "try:\n",
    "    os.makedirs(filename)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(f'{filename}/config.json', 'w') as f:\n",
    "    json.dump(args.__dict__, f, indent=2)\n",
    "\n",
    "models = ['distilbert-base-uncased-imdb-saved',\n",
    "     'bert-base-uncased-imdb-saved',\n",
    "     'roberta-base-imdb-saved',\n",
    "     'distilbert-base-uncased-md_gender_bias-saved',\n",
    "     'bert-base-uncased-md_gender_bias-saved',\n",
    "     'roberta-base-md_gender_bias-saved',\n",
    "     'bert-base-uncased-s2d-saved',\n",
    "     'distilbert-base-uncased-s2d-saved',\n",
    "     'roberta-base-s2d-saved']\n",
    "\n",
    "if args.model.replace('thaile/','') not in models:\n",
    "    print(\"CAUTION! You are running a model not in the model cards.\")\n",
    "\n",
    "try:\n",
    "    from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "    model = ORTModelForSequenceClassification.from_pretrained(args.model, \n",
    "                                                            export=True, \n",
    "                                                            provider=\"CUDAExecutionProvider\", \n",
    "                                                            use_io_binding=True)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model, use_fast=True)\n",
    "except:\n",
    "    print(\"Error using Optimum Runtime, using default model settings\")\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(args.model) \n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model)\n",
    "\n",
    "\n",
    "if args.max_length:\n",
    "    tokenizer.model_max_length = args.max_length\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "outputName = \"output\"\n",
    "startIndex = 0\n",
    "csvName = outputName + str(startIndex) + \"_log.csv\"\n",
    "folderName = \"outputName\" + str(startIndex)\n",
    "\n",
    "_, dataset_test, categories = load_dataset_custom(args.dataset, args.seed_dataset)\n",
    "dataset = textattack.datasets.HuggingFaceDataset(dataset_test)\n",
    "print(categories)\n",
    "\n",
    "dataset = dataset._dataset\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "data = []\n",
    "for i in range(len(dataset)):\n",
    "    text = dataset[i].get(args.text_col)\n",
    "    example = textattack.shared.attacked_text.AttackedText(text)\n",
    "    num_words_non_stopwords = len([w for w in example._words if w not in stopwords])\n",
    "    if args.min_length and num_words_non_stopwords < args.min_length:\n",
    "        continue\n",
    "    if args.max_length and example.num_words > args.max_length:\n",
    "        continue\n",
    "    label = dataset[i].get(args.label_col)\n",
    "    data.append((example, label))\n",
    "\n",
    "categories = list(np.unique([tmp[1] for tmp in data]))\n",
    "print(\"CATEGORIES\", categories)\n",
    "\n",
    "if args.num > 0:\n",
    "    rng = np.random.default_rng(seed=args.seed_dataset)\n",
    "    rng.shuffle(data)\n",
    "    data = data[:args.num]\n",
    "\n",
    "pbar = tqdm(range(0, len(data)), bar_format='{desc:<20}{percentage:3.0f}%|{bar:10}{r_bar}')\n",
    "\n",
    "\n",
    "def generate_attacker(ATTACK_CLASS, args, custom_seed=None, greedy_search=True):\n",
    "    \n",
    "    if args.lime_sr is not None:\n",
    "        samples = args.lime_sr\n",
    "    elif args.dataset == 'imdb':\n",
    "        samples = 4500\n",
    "    elif args.dataset =='gb':\n",
    "        samples = 1500\n",
    "    elif args.dataset == 's2d':\n",
    "        samples = 2500\n",
    "    else:\n",
    "        samples = 5000\n",
    "        \n",
    "    attack = ATTACK_CLASS.build(model_wrapper,\n",
    "                                categories = categories,\n",
    "                                featureSelector = args.top_n, \n",
    "                                limeSamples = samples,\n",
    "                                random_seed = args.seed if not custom_seed else custom_seed,\n",
    "                                success_threshold=args.success_threshold,\n",
    "                                model_batch_size=args.batch_size,\n",
    "                                max_candidates=args.max_candidate,\n",
    "                                logger=pbar if args.debug else None,\n",
    "                                modification_rate=args.modify_rate,\n",
    "                                rbo_p = args.rbo_p,\n",
    "                                similarity_measure=args.similarity_measure,\n",
    "                                greedy_search=greedy_search,\n",
    "                                search_method = args.method,\n",
    "                                crossover = args.crossover,\n",
    "                                parent_selection = args.parent_selection\n",
    "                                )\n",
    "\n",
    "    attack_args = textattack.AttackArgs(num_examples=1,\n",
    "                                        random_seed=args.seed if not custom_seed else custom_seed,\n",
    "                                        log_to_csv=csvName, \n",
    "                                        checkpoint_interval=250, \n",
    "                                        checkpoint_dir=\"./checkpoints\", \n",
    "                                        disable_stdout=False,\n",
    "                                        )\n",
    "\n",
    "    attacker = textattack.Attacker(attack, textattack.datasets.Dataset([]), attack_args)\n",
    "\n",
    "    return attacker\n",
    "\n",
    "\n",
    "if args.method == \"xaifooler\":\n",
    "    attacker = generate_attacker(ADV_XAI_Attack, args, custom_seed=None)\n",
    "\n",
    "elif args.method == \"inherent\":\n",
    "    attacker1 = generate_attacker(ADV_XAI_Attack, args, custom_seed=np.random.choice(1000))\n",
    "    attacker2 = generate_attacker(ADV_XAI_Attack, args, custom_seed=np.random.choice(1000))\n",
    "\n",
    "elif args.method == \"random\":\n",
    "    attacker = generate_attacker(RANDOM_BASELINE_Attack, args, custom_seed=None, greedy_search=True)\n",
    "\n",
    "elif args.method == \"truerandom\":\n",
    "    attacker = generate_attacker(RANDOM_BASELINE_Attack, args, custom_seed=None, greedy_search=False)\n",
    "\n",
    "elif args.method == 'ga':\n",
    "    attacker = generate_attacker(ADV_XAI_Attack, args, custom_seed=None)\n",
    "\n",
    "results = []\n",
    "\n",
    "if not args.rerun:\n",
    "    previous_results = load(filename)\n",
    "    if previous_results:\n",
    "        print(\"LOADED PREVIOUS RESULTS\", len(previous_results))\n",
    "        previous_texts = set([result['example'].text for result in previous_results if not result['log']])\n",
    "        print(previous_texts)\n",
    "        results = previous_results\n",
    "\n",
    "rbos = []\n",
    "sims = []\n",
    "for i in pbar:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    # try:\n",
    "        example, label = data[i]\n",
    "        print(\"****TEXT*****\")\n",
    "        print(\"Text\", example.text)\n",
    "        print(\"Label\", label)\n",
    "        #print(\"# words (ignore stopwords)\", example.num_words_non_stopwords)\n",
    "        num_words_non_stopwords = len([w for w in example._words if w not in stopwords])\n",
    "        print(\"# words (ignore stopwords)\", num_words_non_stopwords)\n",
    "        \n",
    "        if not args.rerun and previous_results and example.text in previous_texts:\n",
    "            print(\"ALREADY DONE, IGNORE...\")\n",
    "            continue\n",
    "        # #soft split\n",
    "        # if args.max_length:\n",
    "        #     text = \" \".join(text.split()[:args.max_length])\n",
    "\n",
    "        if args.method in set([\"xaifooler\", \"random\", \"truerandom\",'ga']):\n",
    "            output = attacker.attack.goal_function.get_output(example)\n",
    "            result = None\n",
    "            \n",
    "            #certain malformed instances can return empty dataframes\n",
    "            \n",
    "            #result = attacker.attack.attack(example, output)\n",
    "\n",
    "            try:\n",
    "                result = attacker.attack.attack(example, output)\n",
    "            except:\n",
    "                print(\"Error generating result\")\n",
    "                results.append({'example': example, 'result': None, 'exp_before': None, 'exp_after': None, 'rbo': None, 'log': 'prediction mismatched'})\n",
    "                if not args.debug:\n",
    "                    save(results, filename)\n",
    "                continue\n",
    "                \n",
    "            if result:\n",
    "                print(result.__str__(color_method=\"ansi\") + \"\\n\")\n",
    "\n",
    "                sent1 = result.original_result.attacked_text.text\n",
    "                sent2 = result.perturbed_result.attacked_text.text\n",
    "\n",
    "                exp1 = attacker.attack.goal_function.generateExplanation(sent1)\n",
    "                exp2 = attacker.attack.goal_function.generateExplanation(sent2)\n",
    "\n",
    "            else:\n",
    "                print(\"PREDICTION MISMATCHED WITH EXPLANTION\")\n",
    "                results.append({'example': example, 'result': None, 'exp_before': None, 'exp_after': None, 'rbo': None, 'log': 'prediction mismatched'})\n",
    "                if not args.debug:\n",
    "                    save(results, filename)\n",
    "                continue\n",
    "\n",
    "        elif args.method == \"inherent\":\n",
    "            result = None\n",
    "\n",
    "            sent1 = example.text\n",
    "            sent2 = example.text\n",
    "\n",
    "            exp1 = attacker1.attack.goal_function.generateExplanation(sent1)\n",
    "            exp2 = attacker2.attack.goal_function.generateExplanation(sent2)\n",
    "\n",
    "        print(\"Base prediction\", exp1[1])\n",
    "        print(\"Attacked prediction\", exp2[1])\n",
    "        print(\"sent1\", sent1)\n",
    "        print(\"sent2\", sent2)\n",
    "\n",
    "        df1 = format_explanation_df(exp1[0], target=exp1[1])\n",
    "        df2 = format_explanation_df(exp2[0], target=exp2[1])\n",
    "        print(df1)\n",
    "        print(df2)\n",
    "\n",
    "        targetList = df2.get('feature').values\n",
    "        baseList = df1.get('feature').values\n",
    "\n",
    "        rboOutput = RBO(targetList, baseList, p=args.rbo_p)\n",
    "        print(\"rboOutput\", rboOutput)\n",
    "        rbos.append(rboOutput)\n",
    "        \n",
    "        simOutput = generate_comparative_similarities(result.perturbed_result.attacked_text.text,exp1,exp2)\n",
    "        print(\"Comparative Sims\", simOutput)\n",
    "        sims.append(simOutput)\n",
    "        # pbar.set_description(f\"#{i} | Text: {text[:20]}... | RBO Score: {round(rboOutput,2)}\")\n",
    "        pbar.set_description('||Average RBO={}||'.format(np.mean(rbos)))\n",
    "\n",
    "\n",
    "        pwp = 0\n",
    "        adjusted_length = 0\n",
    "        s1 = result.original_result.attacked_text.text.split() \n",
    "        s2 = result.perturbed_result.attacked_text.text.split()\n",
    "\n",
    "        for i in range(len(s1)):\n",
    "            #print(\"Comparing: \", s1[i] , s2[i])\n",
    "            if s1[i][0].isalpha():  \n",
    "                if s1[i] != s2[i]:\n",
    "                    pwp += 1\n",
    "            else:\n",
    "                #print(s1[i], \" is non alphanumeric\")\n",
    "                adjusted_length += 1\n",
    "        #print(pwp,len(s1),adjusted_length)\n",
    "        pwp = pwp / (len(s1)-adjusted_length)\n",
    "        print(\"Perturbed Word Proportion: \",pwp)\n",
    "\n",
    "        results.append({'example': example, 'result': result, 'exp_before': exp1, 'exp_after': exp2, 'rbo': rboOutput,'comparativeSims': simOutput, 'log': None,'perturbed_word_proportion': pwp})\n",
    "\n",
    "\n",
    "        if not args.debug:\n",
    "            save(results, filename)\n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
